{
    "table_list": [
        "Layer Type Self-Attention Recurrent Convolutional Self-Attention (restricted) Complexity per Layer O(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d) Sequential Maximum Path Length Operations O(1) O(n) O(1) O(1) O(1) O(n) O(logk(n)) O(n/r)",
        "Model ByteNet [18] Deep-Att + PosUnk [39] GNMT + RL [38] ConvS2S [9] MoE [32] Deep-Att + PosUnk Ensemble [39] GNMT + RL Ensemble [38] ConvS2S Ensemble [9] Transformer (base model) Transformer (big) BLEU EN-DE EN-FR 23.75 24.6 25.16 26.03 26.30 26.36 27.3 28.4 39.2 39.92 40.46 40.56 40.4 41.16 41.29 38.1 41.8 Training Cost (FLOPs) EN-DE EN-FR 2.3 · 1019 9.6 · 1018 2.0 · 1019 1.8 · 1020 7.7 · 1019 1.0 · 1020 1.4 · 1020 1.5 · 1020 1.2 · 1020 8.0 · 1020 1.1 · 1021 1.2 · 1021 3.3 · 1018 2.3 · 1019",
        "base (A) (B) (C) (D) N dmodel 6 512 2 4 8 256 1024 dff 2048 1024 4096 h 8 1 4 16 32 dk 64 512 128 32 16 16 32 32 128 dv 64 512 128 32 16 32 128 Pdrop 0.1 0.0 0.2 ϵls 0.1 0.0 0.2 PPL train steps (dev) 100K 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 300K 4.33 BLEU params ×106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4 58 60 36 50 80 28 168 53 90 (E) big 6 positional embedding instead of sinusoids 1024 4096 16 0.3 213",
        "Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al. (2006) [29] Zhu et al. (2013) [40] Dyer et al. (2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8] WSJ 23 F1 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3"
    ],
    "table_summaries": [
        "- 何がまとめられているテーブルなのか\n    このテーブルは、異なるレイヤータイプの自己注意ネットワークの計算量と最大のパス長を比較したものです。\n\n- テーブルに記載されているキーワード\n    - レイヤータイプ (Layer Type): 自己注意ネットワークの層のタイプ。\n    - 計算量 (Complexity per Layer): 各層の計算量。\n    - 最大のパス長 (Sequential Maximum Path Length): 各層の最大のパスの長さ。\n\n- テーブルから読み取ることができる分析結果\n    - 自己注意ネットワークの層のタイプによって、計算量と最大のパスの長さが異なる。\n    - 一般的に、計算量と最大のパスの長さは、レイヤーのタイプが複雑になるにつれて増加する。\n    - 自己注意ネットワークの層のタイプを選択する際には、計算量と最大のパスの長さを考慮することが重要である。",
        "**何がまとめられているテーブルなのか**\n- このテーブルは、様々な機械翻訳モデルのBLEUスコアとトレーニングコスト（FLOPs）を比較したものです。\n\n\n**テーブルに記載されているキーワード**\n- BLEU（バイリンガル・評価・アンダー・スコアリング）：機械翻訳の品質を評価するための指標。\n- FLOPs（浮動小数点演算）：機械学習モデルの計算量を測定するための単位。\n\n\n**テーブルから読み取ることができる分析結果**\n- Transformerというモデルは、EN-DEとEN-FRの両方の言語ペアで最も高いBLEUスコアを達成しています。\n- Transformerは、他のモデルよりもトレーニングコストが高い傾向があります。\n- Deep-Att + PosUnk EnsembleとGNMT + RL Ensembleは、単一モデルよりも高いBLEUスコアを達成していますが、トレーニングコストも高くなっています。\n- ConvS2Sは、単一モデルの中でも最も低いBLEUスコアですが、トレーニングコストも最も低いです。",
        "**何がまとめられているテーブルなのか:**\n- このテーブルは、 Transformer モデルのハイパーパラメータと、それらに対するモデルのパフォーマンスをまとめたものです。\n\n**テーブルに記載されているキーワード:**\n- モデル名: Transformer\n- ハイパーパラメータ: dmodel、dff、h、dk、dv、Pdrop、ϵls\n- モデルのパフォーマンス: PPL (perplexity)、BLEU、params\n\n**テーブルから読み取ることができる分析結果:**\n- ハイパーパラメータ dmodel を大きくすると、PPL と BLEU が改善する傾向がある。\n- ハイパーパラメータ dff を大きくすると、PPL と BLEU が改善する傾向がある。\n- ハイパーパラメータ h を大きくすると、PPL が改善する傾向があるが、BLEU は低下する傾向がある。\n- ハイパーパラメータ dk と dv を大きくすると、PPL と BLEU が改善する傾向がある。\n- ハイパーパラメータ Pdrop を大きくすると、PPL が改善する傾向があるが、BLEU は低下する傾向がある。\n- ハイパーパラメータ ϵls を大きくすると、PPL と BLEU が改善する傾向がある。\n- 位置エンコーディングとしてsinusoidsではなく、大きな位置埋め込みを使用すると、PPL と BLEU が改善する傾向がある。",
        "- **何がまとめられているテーブルなのか**: **構文分析**のF1スコアを比較したテーブルです。\n\n\n- **テーブルに記載されているキーワード**:\n    - 構文分析\n    - F1スコア\n    - WSJ\n    - discriminative\n    - semi-supervised\n    - multi-task\n    - generative\n\n\n- **テーブルから読み取ることができる分析結果**:\n    - Vinyals & Kaiser el al. (2014)のモデルは、WSJデータセットでの構文分析タスクで最も高いF1スコアを達成している。\n    - WSJデータセットでの構文分析タスクでは、discriminativeなモデルの方がgenerativeなモデルよりも高いF1スコアを達成している。\n    - WSJデータセットでの構文分析タスクでは、semi-supervisedなモデルの方がsupervisedなモデルよりも高いF1スコアを達成している。\n    - WSJデータセットでの構文分析タスクでは、multi-taskなモデルの方がsingle-taskなモデルよりも高いF1スコアを達成している。\n    - Transformerモデルは、他のモデルよりも高いF1スコアを達成している。"
    ]
}